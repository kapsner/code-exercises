{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f62824b"
      },
      "source": [
        "# Installiert das 'lightning' Paket für das Training des Modells\n",
        "%pip install lightning -qq"
      ],
      "id": "7f62824b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64374f19",
      "metadata": {
        "id": "64374f19"
      },
      "outputs": [],
      "source": [
        "# Importiert die notwendigen Bibliotheken für Datenverarbeitung, Modellierung und Training\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from timm import create_model\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset\n",
        "import lightning as L\n",
        "from torchmetrics.utilities.data import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a407f98a",
      "metadata": {
        "id": "a407f98a"
      },
      "outputs": [],
      "source": [
        "# Lädt das \"chihuahua-muffin\" Dataset von Hugging Face und teilt es in Trainings- und Testsets auf\n",
        "# https://huggingface.co/datasets/sasha/chihuahua-muffin\n",
        "ds = load_dataset(\"sasha/chihuahua-muffin\")\n",
        "print(ds.keys())\n",
        "split_dataset = ds['train'].train_test_split(test_size=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "333c4cec",
      "metadata": {
        "id": "333c4cec"
      },
      "outputs": [],
      "source": [
        "# Definiert die Standard-Transformationsschritte für die Bildvorverarbeitung\n",
        "# Dazu gehören das Konvertieren in einen Tensor, das Ändern der Größe und die Normalisierung\n",
        "DEFAULT_TRANSFORM = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.Normalize(\n",
        "      mean=[0.485, 0.456, 0.406],\n",
        "      std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c580079a",
      "metadata": {
        "id": "c580079a"
      },
      "outputs": [],
      "source": [
        "# Wendet die definierten Transformationen auf das Dataset an\n",
        "split_dataset = split_dataset.map(\n",
        "  lambda example: {'image': DEFAULT_TRANSFORM(example['image'])},\n",
        "  num_proc=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4d8c7fc",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "e4d8c7fc"
      },
      "outputs": [],
      "source": [
        "# Erstellt Data-Loader für Trainings- und Validierungsdaten\n",
        "split_dataset.set_format(type=\"torch\", columns=[\"image\", \"label\"])\n",
        "batch_sizes = (12, 4)\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "  split_dataset['train'],\n",
        "  batch_size=batch_sizes[0],\n",
        "  shuffle=True,\n",
        "  num_workers=4\n",
        ")\n",
        "val_dataloader = torch.utils.data.DataLoader(\n",
        "  split_dataset['test'],\n",
        "  batch_size=batch_sizes[1],\n",
        "  num_workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "136806ee",
      "metadata": {
        "id": "136806ee"
      },
      "outputs": [],
      "source": [
        "# Darstellung von 16 zufälligen Bildern aus dem Original-Dataset\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "random.seed(123) # Seed setzen für Reproduzierbarkeit\n",
        "random_integers = []\n",
        "for _ in range(16):\n",
        "    random_integers.append(random.randint(0, len(ds[\"train\"]) - 1))\n",
        "for i, z in zip(random_integers, range(16)):\n",
        "    fig.add_subplot(4, 4, z+1)\n",
        "    plt.imshow(ds['train'][i][\"image\"])\n",
        "    if ds['train'][i][\"label\"] == 0:\n",
        "        label = 'Muffin'\n",
        "    else:\n",
        "        label = 'Chihuahua'\n",
        "    plt.xlabel(f'{label}')\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f313fda",
      "metadata": {
        "id": "5f313fda"
      },
      "outputs": [],
      "source": [
        "# Darstellung der ersten 16 Trainings-Bilder aus dem \"split_dataset\" (auf diese\n",
        "# Bilder wurden bereits die Tranformationen (\"DEFAULT_TRANSFORMATIONS\") angewendet)\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "for i in range(16):\n",
        "    fig.add_subplot(4, 4, i+1)\n",
        "    plt.imshow(\n",
        "        np.transpose(\n",
        "            split_dataset['train'][i][\"image\"],\n",
        "            (1 , 2, 0)\n",
        "        )\n",
        "    )\n",
        "    if split_dataset['train'][i][\"label\"] == 0:\n",
        "        label = 'Muffin'\n",
        "    else:\n",
        "        label = 'Chihuahua'\n",
        "    plt.xlabel(f'{label}')\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2947b13",
      "metadata": {
        "id": "e2947b13"
      },
      "outputs": [],
      "source": [
        "# Neural Network Module\n",
        "class MuffinChihuahuaClassificator(L.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = create_model('resnet18', pretrained=True, num_classes=1)\n",
        "        self.train_step_outputs = []\n",
        "        self.valid_step_outputs = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def loss(preds, targets):\n",
        "        loss = F.binary_cross_entropy_with_logits(\n",
        "          input=torch.squeeze(preds),\n",
        "          target=targets.type_as(preds)\n",
        "        )\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        return self._shared_step(batch, prefix=\"train\")\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        # https://github.com/Lightning-AI/pytorch-lightning/releases/tag/2.0.0#bc-changes-pytorch\n",
        "        self._shared_epoch_end(prefix=\"train\")\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        return self._shared_step(batch, prefix=\"valid\")\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        self._shared_epoch_end(prefix=\"valid\")\n",
        "\n",
        "    def _shared_step(self, batch, prefix):\n",
        "        x = batch[\"image\"]\n",
        "        y = batch[\"label\"]\n",
        "\n",
        "        # apply model to data\n",
        "        y_hat = self(x)\n",
        "\n",
        "        # batch size\n",
        "        bs = torch.tensor(len(x), dtype=torch.int16).type_as(x)\n",
        "        # loss\n",
        "        loss = self.loss(\n",
        "            preds=y_hat,\n",
        "            targets=y.type(torch.FloatTensor)\n",
        "        )\n",
        "        self.log(\n",
        "            name=f\"{prefix}_loss\",\n",
        "            value=loss,\n",
        "            prog_bar=True,\n",
        "            logger=True,\n",
        "            on_step=True,\n",
        "            on_epoch=False\n",
        "        )\n",
        "        eval(f\"self.{prefix}_step_outputs\").append({\"batch_size\": bs, \"loss\": loss})\n",
        "        return loss\n",
        "\n",
        "    def _shared_epoch_end(self, prefix):\n",
        "        # concat batch sizes\n",
        "        batch_sizes = torch.stack(\n",
        "            [x[\"batch_size\"] for x in eval(f\"self.{prefix}_step_outputs\")]\n",
        "        ).type_as(eval(f\"self.{prefix}_step_outputs\")[0][\"loss\"])\n",
        "\n",
        "        # concat losses\n",
        "        losses = torch.stack(\n",
        "            [x[\"loss\"] for x in eval(f\"self.{prefix}_step_outputs\")]\n",
        "        ).type_as(eval(f\"self.{prefix}_step_outputs\")[0][\"loss\"])\n",
        "\n",
        "        # clear outputs\n",
        "        eval(f\"self.{prefix}_step_outputs\").clear()\n",
        "\n",
        "        # calculating weighted mean loss\n",
        "        avg_loss = torch.sum(losses * batch_sizes) / torch.sum(batch_sizes)\n",
        "\n",
        "        self.log(\n",
        "            name=f\"loss/{prefix}\",\n",
        "            value=avg_loss,\n",
        "            prog_bar=True,\n",
        "            logger=True,\n",
        "            on_step=False,\n",
        "            on_epoch=True\n",
        "        )\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.model.parameters(), lr=0.005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1deaa8b3",
      "metadata": {
        "id": "1deaa8b3"
      },
      "outputs": [],
      "source": [
        "# Dataset Modul\n",
        "class ClassificationData(L.LightningDataModule):\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return train_dataloader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "      return val_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7a35ce0",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "a7a35ce0"
      },
      "outputs": [],
      "source": [
        "# Training des Modells\n",
        "n_epochs = 5 # Testen für 5 Epochen; je nach Bedarf / Verfügbarkeit von Ressourcen anpassen\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = MuffinChihuahuaClassificator()\n",
        "    data = ClassificationData()\n",
        "    trainer = L.Trainer(max_epochs=n_epochs)\n",
        "    trainer.fit(model, data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "115a4edb"
      },
      "source": [
        "# Definiert das Verzeichnis, in dem die TensorBoard-Logs gespeichert werden\n",
        "# Standardmäßig speichert Lightning die Logs in 'lightning_logs'\n",
        "log_dir = trainer.logger.log_dir\n",
        "\n",
        "# Überprüft, ob das Log-Verzeichnis existiert\n",
        "if not os.path.exists(log_dir):\n",
        "    print(f\"Log-Verzeichnis nicht gefunden unter {log_dir}\")\n",
        "else:\n",
        "    # Initialisiert EventAccumulator\n",
        "    event_acc = EventAccumulator(log_dir)\n",
        "    event_acc.Reload()\n",
        "\n",
        "    # Holt die Tags für Trainings- und Validierungsverlust\n",
        "    train_loss_tag = \"loss/train\"\n",
        "    val_loss_tag = \"loss/valid\"\n",
        "\n",
        "    # Überprüft, ob die Tags in den Logs existieren\n",
        "    if train_loss_tag in event_acc.Tags()['scalars'] and val_loss_tag in event_acc.Tags()['scalars']:\n",
        "        # Holt die Verlustwerte für jede Epoche\n",
        "        train_loss_events = event_acc.Scalars(train_loss_tag)\n",
        "        val_loss_events = event_acc.Scalars(val_loss_tag)\n",
        "\n",
        "        # Extrahiert Epoche und Wert für jedes Event\n",
        "        train_epochs = [event.step for event in train_loss_events]\n",
        "        train_values = [event.value for event in train_loss_events]\n",
        "\n",
        "        val_epochs = [event.step for event in val_loss_events]\n",
        "        val_values = [event.value for event in val_loss_events]\n",
        "\n",
        "        # Plottet die Verlustkurven\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(train_epochs, train_values, label=\"Training-loss\")\n",
        "        plt.plot(val_epochs, val_values, label=\"Validation-loss\")\n",
        "        plt.xlabel(\"Epoche\")\n",
        "        plt.ylabel(\"Verlust\")\n",
        "        plt.title(\"Trainings- und Validierungs-Loss-Kurven\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Loss-Tags '{train_loss_tag}' oder '{val_loss_tag}' nicht in den Logs gefunden.\")"
      ],
      "id": "115a4edb",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}