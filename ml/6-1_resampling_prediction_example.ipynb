{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Resampling-Methoden und Vorhersagen\"\n",
        "jupyter: ir\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| include: false\n",
        "# Daten laden (Setup aus ML-Einführung)\n",
        "\n",
        "req_pkg <- c(\n",
        "  \"riskCommunicator\", \"data.table\", \"tinyplot\", \"see\",\n",
        "  \"tidymodels\", \"parsnip\", \"yardstick\", \"parallel\",\n",
        "  \"ranger\", \"rsample\", \"workflows\", \"tune\", \"dials\",\n",
        "  \"bonsai\", \"lightgbm\", \"kdry\"\n",
        ")\n",
        "for (r in req_pkg) {\n",
        "  if (!(r %in% installed.packages()[, \"Package\"])) {\n",
        "    install.packages(r)\n",
        "  }\n",
        "}\n",
        "\n",
        "dataset_full <- riskCommunicator::framingham |>\n",
        "  data.table::data.table() # Daten einlesen\n",
        "# Subset: Basisuntersuchung\n",
        "dataset <- dataset_full[get(\"PERIOD\") == 1, ]\n",
        "\n",
        "# Relevante Spalten definieren\n",
        "use_cols <- c(\"SEX\", \"TOTCHOL\", \"AGE\", \"SYSBP\",\n",
        "\"CURSMOKE\", \"CIGPDAY\", \"BMI\", \"DIABETES\",\n",
        "\"HYPERTEN\")\n",
        "# Relevante Spalten filtern, fehlende Werte entfernen\n",
        "dataset <- dataset[\n",
        "  , .SD, .SDcols = use_cols\n",
        "] |>na.omit()\n",
        "\n",
        "# Transformieren der katgeorialen Variablen\n",
        "# \"SEX\" \"CURSMOKE\" \"CIGPDAY\" \"DIABETES\" \"HYPERTEN\"\n",
        "cat_vars <- use_cols[c(1, 5, 8, 9)]\n",
        "# Datentyp \"factor\" ändern\n",
        "dataset[, (cat_vars) := lapply(\n",
        "  X = .SD,\n",
        "  FUN = factor),\n",
        "  .SDcols = cat_vars\n",
        "]\n",
        "rm(dataset_full, cat_vars, use_cols, r, req_pkg)\n",
        "\n",
        "# Teildatensatz für Regressions-Beispiele\n",
        "# Regression: Zielvariable \"SYSBP\" --> Entfernen von \"HYPERTEN\"\n",
        "dataset_reg <- dataset[\n",
        "  , .SD, .SDcols = setdiff(colnames(dataset), \"HYPERTEN\")\n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| include: false\n",
        "# Parallelisieren, wenn möglich\n",
        "nc <- parallel::detectCores()\n",
        "# Verwende Hälfte der verfügbaren CPU threads,\n",
        "# jedoch mindestens 2\n",
        "ncores <- pmax(2, ceiling(nc) / 2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Verwendung des {parsnip} R-Pakets\n",
        "# (aus dem {tidymodels} Framework)\n",
        "# um ein lineares Regressionsmodell\n",
        "# auf Grundlage der R-Basisimplementierung\n",
        "# {stats::lm} zu definieren\n",
        "lm_spec <- parsnip::linear_reg() |>\n",
        "  parsnip::set_engine(\"lm\") |>\n",
        "  parsnip::set_mode(\"regression\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Zusammenfassung des definierten Modells\n",
        "lm_spec |> parsnip::translate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Initialisieren des (Pseudo-)\n",
        "# Zufallszahlengenerators\n",
        "set.seed(123)\n",
        "# Erstellen eines Train-/ Test-Splits\n",
        "# (60% Trainingsdaten, 40% Testdaten)\n",
        "# mithilfe des {rsample} R-Pakets;\n",
        "# Die Daten werden `stratifiziert`\n",
        "# aufgeteilt, sodass die Zielvariable\n",
        "# in beiden Teildatensätzen ähnlich\n",
        "# verteilt ist (`strata=\"SYSBP\"`)\n",
        "init_split <- rsample::initial_split(\n",
        "  data = dataset_reg,\n",
        "  prop = 0.6,\n",
        "  strata = \"SYSBP\"\n",
        ")\n",
        "init_split |> print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Zielvariable: Trainingsdaten\n",
        "# Mittelwert + Standardabweichung\n",
        "rsample::training(init_split)$SYSBP |>\n",
        "  kdry::rep_mean_sd()\n",
        "# Zielvariable: Testdaten\n",
        "# Mittelwert + Standardabweichung\n",
        "rsample::testing(init_split)$SYSBP |>\n",
        "  kdry::rep_mean_sd()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Training des Modells\n",
        "lm_m1 <- lm_spec |>\n",
        "  parsnip::fit(\n",
        "    SYSBP ~ .,\n",
        "    data = rsample::training(init_split)\n",
        ")\n",
        "\n",
        "# Ausgabe der Koeffizienten des\n",
        "# linearen Regressionsmodells\n",
        "lm_m1 |>\n",
        "  parsnip::extract_fit_engine() |>\n",
        "  coef()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Vorhersage der Zielvariable der Testdaten\n",
        "lm_m1_preds <- lm_m1 |>\n",
        "  parsnip::augment(\n",
        "    new_data = rsample::testing(init_split))\n",
        "\n",
        "# Auf den Testdaten berechneter\n",
        "# Vorhersagefehler\n",
        "(lm_m1_rmse <- lm_m1_preds |>\n",
        "  yardstick::rmse(\n",
        "    truth = SYSBP,\n",
        "    estimate = .pred\n",
        "  ))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Initialisieren des (Pseudo-) Zufallszahlen-\n",
        "# generators mit einem anderen Startwert\n",
        "set.seed(1234)\n",
        "# Erstellen eines Train-/ Test-Splits\n",
        "# (60% Trainingsdaten, 40% Testdaten)\n",
        "init_split <- rsample::initial_split(\n",
        "  data = dataset_reg,\n",
        "  prop = 0.6,\n",
        "  strata = \"SYSBP\"\n",
        ")\n",
        "# Inspektion des `rsplit`-Objekts\n",
        "init_split |> print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Zielvariable: Trainingsdaten\n",
        "# Mittelwert + Standardabweichung\n",
        "rsample::training(init_split)$SYSBP |>\n",
        "  kdry::rep_mean_sd()\n",
        "# Zielvariable: Testdaten\n",
        "# Mittelwert + Standardabweichung\n",
        "rsample::testing(init_split)$SYSBP |>\n",
        "  kdry::rep_mean_sd()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Training des Modells\n",
        "lm_m2 <- lm_spec |>\n",
        "  parsnip::fit(\n",
        "    SYSBP ~ .,\n",
        "    data = rsample::training(init_split)\n",
        "  )\n",
        "\n",
        "# Ausgabe der Koeffizienten des\n",
        "# linearen Regressionsmodells\n",
        "lm_m2 |>\n",
        "  parsnip::extract_fit_engine() |>\n",
        "  coef()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Vorhersagefehler auf den im\n",
        "# Modelltraining ausgesparten\n",
        "# Testdaten\n",
        "lm_m2_preds <- lm_m1 |>\n",
        "  parsnip::augment(\n",
        "    new_data = rsample::testing(init_split)\n",
        "  )\n",
        "\n",
        "# Auf den Testdaten berechneter\n",
        "# Vorhersagefehler\n",
        "(lm_m2_rmse <- lm_m2_preds |>\n",
        "  yardstick::rmse(\n",
        "    truth = SYSBP,\n",
        "    estimate = .pred\n",
        "  ))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Erstellen eines Trainings- (60%),\n",
        "# Validierungs- (20%) und Test-Splits (20%)\n",
        "set.seed(123)\n",
        "init_val_split <- rsample::initial_validation_split(\n",
        "  data = dataset_reg,\n",
        "  prop = c(0.6, 0.2),\n",
        "  strata = \"SYSBP\",\n",
        "  breaks = 4,\n",
        "  pool = 0.1\n",
        ")\n",
        "# Inspektion des `rsplit`-Objekts\n",
        "init_val_split |> print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Training des Modells\n",
        "lm_m3 <- lm_spec |>\n",
        "  parsnip::fit(\n",
        "    SYSBP ~ .,\n",
        "    data = rsample::training(init_val_split)\n",
        "  )\n",
        "\n",
        "# Ausgabe der Koeffizienten des\n",
        "# linearen Regressionsmodells\n",
        "lm_m3 |>\n",
        "  parsnip::extract_fit_engine() |>\n",
        "  coef()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Vorhersage der Zielvariable der\n",
        "# Validierungsdaten\n",
        "lm_m3_preds_val <- lm_m3 |>\n",
        "  parsnip::augment(\n",
        "    new_data = rsample::validation(\n",
        "      init_val_split\n",
        "  ))\n",
        "\n",
        "# Auf den Validierungsdaten berechneter\n",
        "# Vorhersagefehler\n",
        "lm_m3_preds_val |>\n",
        "  yardstick::rmse(\n",
        "    truth = SYSBP,\n",
        "    estimate = .pred\n",
        "  )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Vorhersage der Zielvariable der Testdaten\n",
        "lm_m3_preds_test <- lm_m3 |> parsnip::augment(\n",
        "  new_data = rsample::testing(\n",
        "    init_val_split\n",
        "))\n",
        "\n",
        "# Auf den Testdaten berechneter Vorhersagefehler\n",
        "(lm_m3_rmse <- lm_m3_preds_test |> yardstick::rmse(\n",
        "  truth = SYSBP,\n",
        "  estimate = .pred\n",
        "))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Definition der LOOCV\n",
        "loocv_folds <- rsample::loo_cv(\n",
        "  data = rsample::training(init_split)\n",
        ")\n",
        "loocv_folds |> print(n = 6)\n",
        "\n",
        "# Erstellen eines Workflows mit dem\n",
        "# besten Hyperparameter-Setting\n",
        "lm_m3_wf <- workflows::workflow() |>\n",
        "  workflows::add_model(lm_spec) |>\n",
        "  workflows::add_formula(SYSBP ~ .)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Definition einer Funktion, welche anschließend\n",
        "# mit der via `lapply` definierten For-Schleife\n",
        "# verwendet werden kann\n",
        "fitting_function <- function(i) {\n",
        "  # Speichern der aktuellen Zeile des `rsplit`-Objekts\n",
        "  spr <- rsample::get_rsplit(x = loocv_folds, index = i)\n",
        "  lm_m3_i <- lm_m3_wf |> # fit workflow\n",
        "    parsnip::fit(data = rsample::training(spr))\n",
        "  lm_m3_i |> # predict test observation, return rmse\n",
        "    parsnip::augment(new_data = rsample::testing(spr)) |>\n",
        "    yardstick::rmse(truth = SYSBP, estimate = .pred)\n",
        "}\n",
        "\n",
        "# Ausführen der LOOCV via `lapply` For-Schleife\n",
        "loocv_results <- lapply(\n",
        "  X = seq_len(nrow(loocv_folds)), FUN = fitting_function\n",
        ")\n",
        "\n",
        "# Zusammenfassen aller Einzelergebnisse in einer Tabelle\n",
        "loocv_res <- do.call(rbind, loocv_results)\n",
        "# Berechnung des Mittelwerts über alle Resampling-Ergebnisse\n",
        "loocv_res[[\".estimate\"]] |> mean()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "set.seed(123)\n",
        "# 3-fach wiederholte Kreuzvalidierung\n",
        "cv_folds <- rsample::vfold_cv(\n",
        "  data = rsample::training(init_split),\n",
        "  v = 3,\n",
        "  strata = \"SYSBP\"\n",
        ")\n",
        "cv_folds |> print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Erstellen eines Workflow-Objekts\n",
        "lm_m4_wf <- workflows::workflow() |>\n",
        "  workflows::add_model(lm_spec) |>\n",
        "  workflows::add_formula(SYSBP ~ .)\n",
        "\n",
        "# Training des Modells mit der zuvor definierten\n",
        "# Resampling-Strategie\n",
        "set.seed(123)\n",
        "lm_m4 <- lm_m4_wf |> tune::fit_resamples(cv_folds)\n",
        "\n",
        "# Mittels 3-facher CV auf Trainingsdaten ermittelter\n",
        "# Schätzer für Vorhersagefehler.\n",
        "# (Zum Vergleich:das mit allen Trainingsdaten\n",
        "# trainierte Modell `lm_m2` hat auf den Testdaten\n",
        "# einen RMSE = 19.6 ergeben)\n",
        "lm_m4 |> tune::collect_metrics()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "set.seed(123)\n",
        "# 10-fach wiederholte 10-fache CV\n",
        "rep_cv_folds <- rsample::vfold_cv(\n",
        "  data = rsample::training(init_split),\n",
        "  v = 10,\n",
        "  repeats = 10,\n",
        "  strata = \"SYSBP\"\n",
        ")\n",
        "rep_cv_folds |> print(n=5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Training des Modells mit der zuvor definierten\n",
        "# Resampling-Strategie\n",
        "set.seed(123)\n",
        "lm_m5 <- lm_m4_wf |> tune::fit_resamples(rep_cv_folds)\n",
        "\n",
        "# Mittels 10-fach wiederholter 10-facher CV auf Trainings-\n",
        "# daten ermittelter Schätzer für Vorhersagefehler.\n",
        "# (Zum Vergleich: das mit allen Trainingsdaten\n",
        "# trainierte Modell `lm_m2` hat auf den Testdaten\n",
        "# einen RMSE = 19.6 ergeben)\n",
        "lm_m5 |> tune::collect_metrics()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "set.seed(123)\n",
        "# Verschachtelte CV:\n",
        "# innen: 5-fache CV; außen: 3-fache CV\n",
        "nested_cv_folds_cv <- rsample::nested_cv(\n",
        "  data = rsample::training(init_split),\n",
        "  outside = rsample::vfold_cv(\n",
        "    v = 3, strata = \"SYSBP\"\n",
        "  ),\n",
        "  inside = rsample::vfold_cv(\n",
        "    v = 5, strata = \"SYSBP\"\n",
        "  )\n",
        ")\n",
        "nested_cv_folds_cv |> print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "set.seed(123)\n",
        "# Leere Liste zum Sammeln der Ergebnisse\n",
        "nestedcv_results_cv <- list()\n",
        "\n",
        "# For-Schleife über die einzelnen CV-Folds\n",
        "set.seed(123)\n",
        "for (i in seq_len(nrow(nested_cv_folds_cv))) {\n",
        "  # Speichern der aktuellen Fold-Aufteilung in `current_fold`\n",
        "  current_fold <- nested_cv_folds_cv[i, ]\n",
        "  lm_m6_i <- lm_m4_wf |> # Durchführen des Bootstrappings\n",
        "    tune::fit_resamples(current_fold[[\"inner_resamples\"]][[1]])\n",
        "  rmse_i <- lm_m6_i |> # Ermitteln der Performance-Metrik\n",
        "    tune::collect_metrics()\n",
        "  nestedcv_results_cv <- c(  # Abspeichern der ermittelten\n",
        "    nestedcv_results_cv,     # Metrik in Ergebnis-Liste\n",
        "    list(rmse_i[1, ])\n",
        "  )\n",
        "}\n",
        "\n",
        "# Zusammenfassen aller Einzelergebnisse in einer Tabelle\n",
        "nestedcv_res_cv <- do.call(rbind, nestedcv_results_cv)\n",
        "\n",
        "# Berechnung des Mittelwerts über alle Resampling-Ergebnisse\n",
        "nestedcv_res_cv[[\"mean\"]] |> mean()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "set.seed(123)\n",
        "# Verschachtelte CV:\n",
        "# innen: Bootstrap CV; außen: 3-fache CV\n",
        "nested_cv_folds_btstrp <- rsample::nested_cv(\n",
        "  data = rsample::training(init_split),\n",
        "  outside = rsample::vfold_cv(\n",
        "    v = 3,\n",
        "    strata = \"SYSBP\"\n",
        "  ),\n",
        "  inside = rsample::bootstraps(\n",
        "    times = 25,\n",
        "    strata = \"SYSBP\"\n",
        "  )\n",
        ")\n",
        "nested_cv_folds_btstrp |> print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Leere Liste zum Sammeln der Ergebnisse\n",
        "nestedcv_results_btstrp <- list()\n",
        "# For-Schleife über die einzelnen CV-Folds\n",
        "set.seed(123)\n",
        "for (i in seq_len(nrow(nested_cv_folds_btstrp))) {\n",
        "  # Speichern der aktuellen Fold-Aufteilung in `current_fold`\n",
        "  current_fold <- nested_cv_folds_btstrp[i, ]\n",
        "  lm_m7_i <- lm_m4_wf |> # Durchführen des Bootstrappings\n",
        "    tune::fit_resamples(current_fold[[\"inner_resamples\"]][[1]])\n",
        "  rmse_i <- lm_m7_i |> # Ermitteln der Performance-Metrik\n",
        "    tune::collect_metrics()\n",
        "  nestedcv_results_btstrp <- c( # Abspeichern der ermittelten\n",
        "    nestedcv_results_btstrp,    # Metrik in Ergebnis-Liste\n",
        "    list(rmse_i[1, ])\n",
        "  )\n",
        "}\n",
        "# Zusammenfassen aller Einzelergebnisse in einer Tabelle\n",
        "nestedcv_res_btstrp <- do.call(rbind, nestedcv_results_btstrp)\n",
        "# Berechnung des Mittelwerts über alle Resampling-Ergebnisse\n",
        "nestedcv_res_btstrp[[\"mean\"]] |> mean()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Bootstrapping mit Ziehen von\n",
        "# 100 voneinander unabhängigen Stichproben\n",
        "set.seed(123)\n",
        "btstrp <- rsample::bootstraps(\n",
        "  data = rsample::training(init_split),\n",
        "  times = 100,\n",
        "  strata = \"SYSBP\"\n",
        ")\n",
        "btstrp |> print(n = 6)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "set.seed(123)\n",
        "# Training des Modells mit der zuvor definierten\n",
        "# Resampling-Strategie\n",
        "lm_m8 <- lm_m4_wf |> tune::fit_resamples(btstrp)\n",
        "\n",
        "# Mittels Bootstrapping (100 Stichproben) auf Trainings-\n",
        "# daten ermittelter Schätzer für Vorhersagefehler.\n",
        "# (Zum Vergleich: das mit allen Trainingsdaten\n",
        "# trainierte Modell `lm_m2` hat auf den Testdaten\n",
        "# einen RMSE = 19.6 ergeben)\n",
        "lm_m8 |> tune::collect_metrics()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "library(bonsai)\n",
        "# Definition eines `lightgbm`-Gardient-Boosters\n",
        "# für eine Regressionsaufgabe und das Tuning\n",
        "# von 4 Parametern\n",
        "lgb_tune_spec <- parsnip::boost_tree(\n",
        "  trees = tune::tune(),\n",
        "  mtry = tune::tune(),\n",
        "  learn_rate = tune::tune(),\n",
        "  tree_depth = tune::tune()\n",
        ") |> parsnip::set_engine(\"lightgbm\") |>\n",
        "  parsnip::set_args(num_threads = ncores) |>\n",
        "  parsnip::set_mode(\"regression\")\n",
        "# Ermitteln der zu optimierenden Parameter\n",
        "# und Speichern in separatem Objekt\n",
        "tune_params <- lgb_tune_spec |>\n",
        "  parsnip::extract_parameter_set_dials()\n",
        "tune_params |> print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Vervollständigung der zu optimierenden\n",
        "# Parameter\n",
        "tune_params_fin <- dials::finalize(\n",
        "  object = tune_params,\n",
        "  x = dataset_reg\n",
        ")\n",
        "# Je zwei Einstellungsmöglichkeiten\n",
        "# pro Hyperparameter (2^4 = 16 Komb.)\n",
        "tune_grid <- tune_params_fin |>\n",
        "  dials::grid_regular(levels = 2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| warning: false\n",
        "# nrow(tune_grid) = 16\n",
        "tune_grid |> print(n = 6)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| warning: false\n",
        "# Erstellen eines Workflow-Objekts\n",
        "lgb_wf <- workflows::workflow() |>\n",
        "  workflows::add_model(lgb_tune_spec) |>\n",
        "  workflows::add_formula(SYSBP ~ .)\n",
        "# Verwendung des RMSE als\n",
        "# Performance-Metrik\n",
        "metr <- yardstick::metric_set(\n",
        "  yardstick::rmse\n",
        ")\n",
        "\n",
        "# Jede Hyperparameter-Einstellung wird\n",
        "# mit einer eine 3-fachen CV getestet\n",
        "set.seed(123)\n",
        "lgb_m1_tune_grid <- lgb_wf |>\n",
        "  tune::tune_grid(\n",
        "    resamples = cv_folds,\n",
        "    grid = tune_grid,\n",
        "    metrics = metr\n",
        "  )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Für unterschiedliche Hyperparameter- Einstellungen jeweils\n",
        "# ermittelte Schätzer den für Vorhersagefehler.\n",
        "lgb_m1_metrics <- lgb_m1_tune_grid |> tune::collect_metrics()\n",
        "lgb_m1_metrics |> print(n = 6)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Hyperparameter-Einstellungen der 5 besten Modelle\n",
        "lgb_m1_tune_grid |> tune::show_best(metric = \"rmse\")\n",
        "\n",
        "# Auswahl des besten Hyperparameter-Settings\n",
        "lgb_m1_tune_grid_best <- lgb_m1_tune_grid |>\n",
        "  tune::select_best(metric = \"rmse\")\n",
        "# Die beste Hyperparameter-Einstellung\n",
        "lgb_m1_tune_grid_best\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| warning: false\n",
        "# Erstellen eines Workflows mit dem\n",
        "# besten Hyperparameter-Setting\n",
        "lgb_m1_final <- lgb_wf |>\n",
        "  tune::finalize_workflow(\n",
        "    lgb_m1_tune_grid_best\n",
        "  )\n",
        "\n",
        "# Training eines finalen Modells mit\n",
        "# der besten Hyperparameter-Einstellung\n",
        "# und automatische Evaluation auf den\n",
        "# ausgesparten Testdaten\n",
        "# (siehe auch `?tune::last_fit`)\n",
        "lgb_m1_final_fit <- lgb_m1_final |>\n",
        "  tune::last_fit(init_split)\n",
        "# Schätzer für Vorhersagefehler auf den\n",
        "# Testdaten für ausgewählte Hyperparameter-\n",
        "# Einstellung\n",
        "lgb_m1_final_fit |>\n",
        "  tune::collect_metrics()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "tune_params_fin <- dials::finalize(\n",
        "  object = tune_params,\n",
        "  x = dataset_reg\n",
        ")\n",
        "\n",
        "# Zufällige Auswahl von 20 Hyperparameter-\n",
        "# Einstellungen\n",
        "tune_grid_random <- tune_params_fin |>\n",
        "  dials::grid_random(size = 20)\n",
        "\n",
        "# nrow(tune_grid_random) = 20\n",
        "tune_grid_random |> print(n = 6)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| warning: false\n",
        "# Jede Hyperparameter-Einstellung wird mit einer 3-fachen CV getestet\n",
        "set.seed(123)\n",
        "lgb_m2_tune_grid_rand <- lgb_wf |>\n",
        "  tune::tune_grid(\n",
        "    resamples = cv_folds,\n",
        "    grid = tune_grid_random,\n",
        "    metrics = metr\n",
        "  )\n",
        "\n",
        "# Für unterschiedliche Hyperparameter-Einstellungen jeweils ermittelte\n",
        "# Schätzer den für Vorhersagefehler.\n",
        "lgb_m2_metrics <- lgb_m2_tune_grid_rand |> tune::collect_metrics()\n",
        "lgb_m2_metrics |> print(n = 6)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Hyperparameter-Einstellungen der 5 besten Modelle\n",
        "lgb_m2_tune_grid_rand |> tune::show_best(metric = \"rmse\")\n",
        "\n",
        "# Auswahl des besten Hyperparameter-Settings\n",
        "lgb_m2_tune_grid_rand_best <- lgb_m2_tune_grid_rand |>\n",
        "  tune::select_best(metric = \"rmse\")\n",
        "\n",
        "# Die beste Hyperparameter-Einstellung\n",
        "lgb_m2_tune_grid_rand_best\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| warning: false\n",
        "# Erstellen eines Workflows mit dem\n",
        "# besten Hyperparameter-Setting\n",
        "lgb_m2_final <- lgb_wf |>\n",
        "  tune::finalize_workflow(\n",
        "    lgb_m2_tune_grid_rand_best\n",
        "  )\n",
        "\n",
        "# Training eines finalen Modells mit\n",
        "# der besten Hyperparameter-Einstellung\n",
        "# und automatische Evaluation auf den\n",
        "# ausgesparten Testdaten\n",
        "lgb_m2_final_fit <- lgb_m2_final |>\n",
        "  tune::last_fit(init_split)\n",
        "\n",
        "# Vorhersagefehler auf den Testdaten\n",
        "lgb_m2_final_fit |>\n",
        "  tune::collect_metrics()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| warning: false\n",
        "# Konfiguration der Bayes'schen Optimierung\n",
        "ctrlb <- tune::control_bayes(verbose = TRUE)\n",
        "set.seed(123)\n",
        "# Über das Argument `initial` kann das Ergebnis\n",
        "# der zuvor durchgeführten Grid-Search\n",
        "# übergeben werden (Vorinformation, die eine\n",
        "# informierte Wahl der a-priori\n",
        "# Wahrscheinlichkeit ermöglicht)\n",
        "lgb_m3_tune_bayes <- lgb_wf |>\n",
        "  tune::tune_bayes(\n",
        "    resamples = cv_folds,\n",
        "    metrics = metr,\n",
        "    initial = lgb_m2_tune_grid_rand,\n",
        "    param_info = tune_params_fin,\n",
        "    iter = 10, # Anzahl an Such-Iterationen\n",
        "    control = ctrlb\n",
        "  )\n",
        "lgb_m3_metrics <- lgb_m3_tune_bayes |>\n",
        "  tune::collect_metrics()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Die ersten 10 Ergebnisse der Bayes'schen Optimierung\n",
        "lgb_m3_metrics |> print(n = 10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Hyperparameter-Einstellungen der 5 besten Modelle\n",
        "lgb_m3_tune_bayes |> tune::show_best(metric = \"rmse\")\n",
        "\n",
        "# Auswahl des besten Hyperparameter-Settings\n",
        "lgb_m3_tune_bayes_best <- lgb_m3_tune_bayes |>\n",
        "  tune::select_best(metric = \"rmse\")\n",
        "\n",
        "# Die beste Hyperparameter-Einstellung\n",
        "lgb_m3_tune_bayes_best\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| warning: false\n",
        "# Erstellen eines Workflows mit dem\n",
        "# besten Hyperparameter-Setting\n",
        "lgb_m3_final <- lgb_wf |>\n",
        "  tune::finalize_workflow(\n",
        "    lgb_m3_tune_bayes_best\n",
        "  )\n",
        "\n",
        "# Training eines finalen Modells mit\n",
        "# der besten Hyperparameter-Einstellung\n",
        "# und automatische Evaluation auf den\n",
        "# ausgesparten Testdaten\n",
        "lgb_m3_final_fit <- lgb_m3_final |>\n",
        "  tune::last_fit(init_split)\n",
        "# Vorhersagefehler auf den Testdaten\n",
        "lgb_m3_final_fit |>\n",
        "  tune::collect_metrics()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "ir",
      "language": "R",
      "display_name": "R",
      "path": "/home/user/.local/share/jupyter/kernels/ir"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}